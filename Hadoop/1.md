## 解压

```bash
tar -zxvf scala-SDK-4.7.0-vfinal-2.12-linux.gtk.x86_64.tar.gz 
```

## install New software

```bash
# PyDev
https://dl.bintray.com/fabioz/pydev/8.0.0/
```

## string sub

```bash
SPARK_HOME
/usr/spark/spark-3.0.1-bin-hadoop2.7

HADOOP_CONF_DIR
/usr/hadoop/hadoop-2.7.3/etc/hadoop

PYSPARK_PYTHON
/home/wfy/anaconda3/bin/python


```

## Python链接库

```bash
/home/wfy/anaconda3/lib/python3.7/site-packages


```

## WordCount.py

```python
# -*- coding:utf-8 -*-
from pyspark import SparkContext
from pyspark import SparkConf


def CreateSparkContext():
    sparkConf = SparkConf().setAppName("WordCounts").set(
        "spark.ui.showConsoleProgress", "false")
    sc = SparkContext(conf=sparkConf)
    print("master="+sc.master)  # 显示当前的运行模式
    SetLogger(sc)  # 设置不显示过多信息，函数在下面
    SetPath(sc)  # 设置文件读取路径，函数在下面
    return (sc)


def SetLogger(sc):  # 去除spark默认显示的杂乱信息
    logger = sc._jvm.org.apache.log4j
    logger.LogManager.getLogger("org").setLevel(logger.Level.ERROR)
    logger.LogManager.getLogger("akka").setLevel(logger.Level.ERROR)
    logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)


def SetPath(sc):  # 配置读取文件的路径（本地文件和hdfs文件）
    global Path
    if sc.master[0:5] == "local":
        # local模式读取本地文件
        Path = "file:/home/wfy/pythonworkspace/PythonProject/"
    else:  # 其他模式（yarn等）读取hdfs文件
        Path = "hdfs://localhost:9000/user/wfy/"


if __name__ == "__main__":
    print("开始执行 wordcount")
    sc = CreateSparkContext()
    print("开始读取文本文件...")
    textFile = sc.textFile(Path+"data/README.md")
    print("文本共有"+str(textFile.count())+"行")
    # map-reduce 运算
    countsRDD = textFile.flatMap(lambda line: line.split(' ')).map(
        lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)
    print("文本统计共有"+str(countsRDD.count())+"项数据")
    # 保存结果到文件
    print("开始保存到文本文件...")
    try:
        countsRDD.saveAsTextFile(Path+"data/output")
    except Exception as e:
        print("输出目录已存在，请先删除原有目录")
    sc.stop()

```



## 环境变量

```bash
SPARK_HOME
${SPARK_HOME}

HADOOP_CONF_DIR
${HADOOP_CONF_DIR}


```



```bash
mkdir -p ~/pythonworkspace/PythonProject/data
cp /usr/spark/spark-3.0.1-bin-hadoop2.7/README.md ~/pythonworkspace/PythonProject/data
#创建HDFS测试目录
hadoop fs -mkdir -p /user/wfy/data
#从本地复制测试文件到HDFS
hadoop fs -copyFromLocal /usr/spark/spark-3.0.1-bin-hadoop2.7/README.md /user/wfy/data/README.md
#查看HDFS测试文件
hadoop fs -ls /user/wfy/data/README.md

# spark-submit --driver-memory 2g --master local[*] /root/pythonworkspace/PythonProject/WordCount.py


spark-submit --driver-memory 2g --master local[*] WordCount.py


HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.3/etc/hadoop spark-submit --driver-memory 2g --executor-cores 2 --master yarn --deploy-mode client WordCount.py

```



