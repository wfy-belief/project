## 时间段

| 名称                   | 时间点 |
| ---------------------- | ------ |
| 管理员                 | 0.03   |
| ssh与免密              | 0.25   |
| jdk                    | 2.15   |
| Hadoop                 | 3.57   |
| wordcount              | 9.58   |
| scala                  | 14.30  |
| spark                  | 16.22  |
| pyspark                | 17.30  |
| python pip换源与包安装 | 22.06  |
| pyspark yarn模式       | 25.50  |
| pyspark yarn模式运行   | 39.10  |
| spark-shell            | 39.55  |
| 重启之后如何重置服务   | 41.20  |



<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?aid=202303417&cid=241794546&page=1&as_wide=1&high_quality=1&danmaku=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>
</div>




[FTP SCALA SPARK 下载地址](ftp://10.61.18.128/%BB%B7%BE%B3%B9%A4%BE%DF-%C8%AB%B2%BF/)

FTP 账号密码是 `WFY` 大写

推荐使用内网FTP下载，不行的话，转到[阿里云](https://mirrors.aliyun.com/apache/)镜像下载。

## 软件版本说明

- **<font color='#e41749'>请查看对应版本问题，版本不符合造成的问题，请百度解决，→假期期间我要出去玩←.</font>**
- 使用的是虚拟机 `VMware Workstation Pro` 
- 环境变量为 `/etc/profile` 每次启动 `Terminal` 都需要 `source`
- 截图太占地方，不截图了，可以直接对照视频。

| 软件名称(software) | 版本信息(version)         |
| ------------------ | ------------------------- |
| jdk(JAVA)          | jdk-8u171-linux-x64       |
| Hadoop(2.7+都可以) | hadoop-2.7.3              |
| scala              | scala-2.12.12             |
| spark              | spark-3.0.1-bin-hadoop2.7 |
| python             | 3.8.2                     |

## VM安装前期准备

**<font color='red'>Vbox 直接跳过。</font>**

[参考文档](https://blog.csdn.net/weixin_43906799/article/details/108552012)

```bash
sudo apt-get autoremove open-vm-tools
sudo apt-get install open-vm-tools-deskto 
```

## hostname

```bash
vim /etc/hostname
```

## 管理员权限

```bash
sudo passwd root
```

## ssh & 免密登陆

```bash
sudo apt-get install openssh-server
sudo /etc/init.d/ssh start
ps -ef|grep ssh

ssh-keygen

cat id_rsa.pub >> authorized_keys
```

## 防火墙

```bash
ufw disable
```

## jdk

设置JDK安装路径

```bash
export JAVA_HOME=/usr/java/jdk1.8.0_171
export JAVA_BIN=$JAVA_HOME/bin
export JAVA_LIB=$JAVA_HOME/lib
export CLASSPATH=.:$JAVA_LIB/tools.jar:$JAVA_LIB/dt.jar
export PATH=$JAVA_HOME/bin:$PATH
```

## Hadoop

### 环境变量

```bash
#设置HADOOP_HOME为Hadoop的安装路径/usr/local/hadoop
export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3
# 设置PATH
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
# 设置HADOOP的其他环境变量
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
# 链接库的相关设置
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
```

### hadoop-env.sh

```bash
vim hadoop-env.sh

/usr/java/jdk1.8.0_171
```

### core-site.xml

```bash
vim core-site.xml

<property>
    <name>hadoop.tmp.dir</name>
    <value>file:/usr/local/hadoop/tmp</value>
    <description>Abase for other temporary directories.</description>
</property>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
</property>
```

### hdfs-site.xml

```bash
vim hdfs-site.xml

<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/name</value>
</property>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/data</value>
</property>
```

### vim yarn-site.xml

```bash
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
    <name>yarn.resourcemanager.address</name>
    <value>127.0.0.1:8032</value>
</property>
<property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>127.0.0.1:8030</value>
</property>
<property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>127.0.0.1:8031</value>
</property>
```

### mapred-site.xml

```bash
cp mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
```

### 格式化

```bash
hadoop namenode -format
```

### 启动

```bash
start-all.sh
```

## wordcount

### 输入

```bash
mkdir -p ~/wordcount/input
cd ~/wordcount

vim WordCount.java

export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
```
### 编译

```bash
hadoop com.sun.tools.javac.Main WordCount.java
jar cf wc.jar WordCount*.class

```

### 测试

```bash
cp /usr/hadoop/hadoop-2.7.3/LICENSE.txt ~/wordcount/input

# 存在不用创建
hadoop fs -mkdir -p /user/wfy/wordcount/input

# 上传
hadoop fs -copyFromLocal LICENSE.txt /user/wfy/wordcount/input

# 列出内容
hadoop fs -ls /user/wfy/wordcount/input

# 运行和输出
cd ..
hadoop jar wc.jar WordCount /user/wfy/wordcount/input/LICENSE.txt  /user/wfy/wordcount/output

# 输出
hadoop fs -ls /user/wfy/wordcount/output

# 查看
hadoop fs -cat /user/wfy/wordcount/output/part-r-00000
```

### 示例代码

```java
import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```

## scala

### 环境变量

```bash
#SCALA Variables
export SCALA_HOME=/usr/scala/scala-2.12.12
export PATH=$PATH:$SCALA_HOME/bin
#SCALA Variables
```

### 运行

```bash
scala

# 退出
:q
```

## spark

### 环境变量

```bash
export SPARK_HOME=/usr/spark/spark-3.0.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
```

### pyspark依赖

```bash
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
```

### spark-env.sh

```bash
SPARK_LOCAL_IP=localhost
```

### Unable to load native-hadoop library

```bash
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH
```

### log4j.properties

```bash
cp log4j.properties.template log4j.properties

log4j.rootCategory=WARN, console
```

### 测试

**<font color='red'>注意：需要先启动hadoop集群</font>**

**<font color='red'>第二个文件如果没有请查询相关内容创建</font>**

```bash
textFile=sc.textFile("file:/usr/spark/spark-3.0.1-bin-hadoop2.7/README.md")
textFile.count()
textFile = sc.textFile("hdfs://localhost:9000/user/wfy/wordcount/input/LICENSE.txt")
textFile.count()
```

## python

### 安装

```bash
apt update
apt install python3-pip
```

### 换源

用户目录创建 .pip

```bash
mkdir ~/.pip
vim ~/.pip/pip.conf

[global]
timeout = 60000
# index-url = https://pypi.tuna.tsinghua.edu.cn/simple
index-url = https://pypi.doubanio.com/simple/
[install]
# trusted-host=mirrors.aliyun.com
trusted-host=pypi.douban.com
```

### 安装

```bash
pip3 install numpy
pip3 install pandas
pip3 install matplotlib
```

## 在yarn上运行pyspark

**<font color='red'>注意：需要先启动hadoop集群，才可以执行该操作</font>**

### yarn-site.xml

```bash
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
 <property>
      <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>4</value>
      <description>Ratio between virtual memory to physical memory when setting memory limits for containers</description>
   </property>
```

### yarn-env.sh

```bash
export JAVA_HOME=/usr/java/jdk1.8.0_171
```

### 解决jar相关问题

```bash
hadoop fs -mkdir -p /user/spark/spark_jars 
hadoop fs -put /usr/spark/spark-3.0.1-bin-hadoop2.7/jars/* /user/spark/spark_jars/ 
```



### spark-defaults.conf

**<font color='red'>注意：此处需要更改为对应的hostname，也就@后面的字符</font>**

```bash
cp spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf

spark.yarn.jars  hdfs://localhost:9000/user/spark/spark_jars/*
spark.master  spark://wfy:7077
```



### spark-env.sh

```bash
export JAVA_HOME=/usr/java/jdk1.8.0_171
export SPARK_HOME=/usr/spark/spark-3.0.1-bin-hadoop2.7
export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3
export HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.3/etc/hadoop/
export SCALA_HOME=/usr/scala/scala-2.12.12
export SPARK_MASTER_IP=localhost
#配置master节点的主机，单机用localhost即可
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_PORT=8888
export SPARK_WORKER_CORES=1
#worknode分成几个核给spark用
export SPARK_EXECUTOR_CORES=1
export SPARK_EXECUTOR_MEMORY=1G
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$HADOOP_HOME/lib/native
export SPARK_LOCAL_IP=localhost
```

### yarn-运行

**<font color='red'>需要重新启动Hadoop服务。这个地方会警告，但是可以消除</font>**

```bash
HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.3/etc/hadoop pyspark --master yarn --deploy-mode client

jps -lm | grep -i spark

sc.master
```

### slaves

```bash
cp slaves.template slaves
vim slaves
```

### start/stop-all.sh

```bash
mv start-all.sh start-spark-all.sh
mv stop-all.sh stop-spark-all.sh
```

### 启动

**<font color='red'>需要添加 sh </font>**

```bash
sh start-spark-all.sh
```

### spark-shell

```bash
spark-shell
```

**<font color='red'>重启电脑后，先启动Hadoop，在启动spark</font>**

## 安装anaconda

### 安装（用户安装）

```bash
bash Anaconda3-5.3.1-Linux-x86_64.sh -b
```

### 环境变量

```bash
export PATH=/home/wfy/anaconda3/bin:$PATH
export ANACONDA_PATH=/home/wfy/anaconda3

export PYSPARK_DRIVER_PYTHON=$ANACONDA_PATH/bin/ipython
export PYSPARK_PYTHON=$ANACONDA_PATH/bin/python

```

### ipynotebook

```bash
mkdir -p ~/pythonwork/ipynotebook
cd ~/pythonwork/ipynotebook

# export PYSPARK_DRIVER_PYTHON_OPTS=" --ip=0.0.0.0 --port=8888"
PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook  --allow-root" pyspark

```

```bash
PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook  --allow-root" HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.3/etc/hadoop pyspark --master yarn --deploy-mode client
```

